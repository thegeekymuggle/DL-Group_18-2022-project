{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code done by,\n",
    "  Pratik Shetty AI21MTECH12005\n",
    "  Varshita Sharma AI21MTECH14009\n",
    "  Sunamdha Harini AI21MTECH14002\n",
    "  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WR8xssmrqE1c",
    "outputId": "b053d106-30f8-43c9-823a-61b47e40eecc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pratik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence \n",
    "from keras.preprocessing import text as tt\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df=pd.read_excel('/content/drive/My Drive/Imocap_text/processed_tran.xlsx')\n",
    "df_label=pd.read_excel('scrap_label.xlsx')\n",
    "# df_label_enc = pd.read_excel('scrap_label_enc.xlsx')\n",
    "df_text=pd.read_excel('scrap_text.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2233\n"
     ]
    }
   ],
   "source": [
    "docs=[]\n",
    "# for text,label in zip(df['text'],df['label']):\n",
    "#   if(label!=-1):\n",
    "#     docs.append({'text':text,'label':label})\n",
    "# print(len(docs))\n",
    "needed_labels = ['neu', 'sad' , 'hap' , 'ang']\n",
    "# docs = np.array([])\n",
    "\n",
    "for sessionid, label in zip(df_label['sessionID'] ,df_label['label']):\n",
    "#   print(label)\n",
    "  if label not in needed_labels:\n",
    "    continue \n",
    "  text = df_text[df_text['sessionID']==sessionid]['text']\n",
    "  if len(text.values) == 0:\n",
    "    continue\n",
    "  else:\n",
    "    text = text.values[0]\n",
    "  label_enc = needed_labels.index(label)\n",
    "#   print(label_enc)\n",
    "  docs.append({'text': text , 'label' : label_enc})\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of items for train  2009\n",
      "no of items for test  224\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(docs)\n",
    "random.shuffle(docs)\n",
    "random.shuffle(docs)\n",
    "total_length=len(docs)\n",
    "train_length=int(.9*total_length)\n",
    "train_list=docs[0:train_length]\n",
    "test_list=docs[train_length:]\n",
    "print('no of items for train ',len(train_list))\n",
    "print('no of items for test ',len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = []\n",
    "y_train_list = []\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for train_l in train_list :\n",
    "  X_train_list.append(train_l['text'])\n",
    "  y_train_list.append(train_l['label'])\n",
    "\n",
    "  \n",
    "for test_l in test_list :\n",
    "  X_test_list.append(test_l['text'])\n",
    "  y_test_list.append(test_l['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train_list)\n",
    "y_train = np.array(y_train_list)\n",
    "\n",
    "X_test = np.array(X_test_list)\n",
    "y_test = np.array(y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bVppv_qhxQhF"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "train_corpus = []\n",
    "for i in range(0, len(X_train)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', X_train[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    train_corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mr3BDUG6yL8T"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "test_corpus = []\n",
    "for i in range(0, len(X_test)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', X_test[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    test_corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uqZMbbdsy0Ay"
   },
   "outputs": [],
   "source": [
    "# print(\"Beforeeaning Data\")\n",
    "X_train = np.array(train_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jEmYnrqmzPih",
    "outputId": "c079cb0d-81ca-46d3-b472-6f1ad32e52d7"
   },
   "outputs": [],
   "source": [
    "X_test = np.array(test_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KtW2Bh_UanYw"
   },
   "outputs": [],
   "source": [
    "y_train_enc_nn = np_utils.to_categorical(y_train)\n",
    "# y_cv_enc_nn = np_utils.to_categorical(y_cv_enc)\n",
    "y_test_enc_nn = np_utils.to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "otBqIIeLeMFu"
   },
   "outputs": [],
   "source": [
    "token = tt.Tokenizer(num_words=None)\n",
    "\n",
    "max_len = 200\n",
    "a = X_train.tolist()+ X_test.tolist()\n",
    "token.fit_on_texts(a)\n",
    "\n",
    "xtrain_seq = token.texts_to_sequences(X_train)\n",
    "# xvalid_seq = token.texts_to_sequences(X_cv['clean_utterance'])\n",
    "xtest_seq = token.texts_to_sequences(X_test)\n",
    "\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "# xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "xtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qUoMC6B1fof-",
    "outputId": "e85b2d5e-38b8-4999-a3bd-e65c5ca251ca"
   },
   "outputs": [],
   "source": [
    "# embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "# for word, i in tqdm(word_index.items()):\n",
    "#     embedding_vector = word_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 200, 150)          217950    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 200)               280800    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 804       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 499,554\n",
      "Trainable params: 499,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 150\n",
    "lstm_out = 200\n",
    "max_features = 200\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, embed_dim,input_length = xtrain_pad.shape[1]))\n",
    "model.add(LSTM(lstm_out, dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(4,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "81/81 [==============================] - 40s 500ms/step - loss: 0.3061 - accuracy: 0.8701 - val_loss: 1.5475 - val_accuracy: 0.6295\n",
      "Epoch 2/300\n",
      "81/81 [==============================] - 42s 514ms/step - loss: 0.3008 - accuracy: 0.8701 - val_loss: 1.5731 - val_accuracy: 0.6027\n",
      "Epoch 3/300\n",
      "81/81 [==============================] - 40s 499ms/step - loss: 0.2908 - accuracy: 0.8736 - val_loss: 1.6153 - val_accuracy: 0.6071\n",
      "Epoch 4/300\n",
      "81/81 [==============================] - 40s 493ms/step - loss: 0.2893 - accuracy: 0.8771 - val_loss: 1.5738 - val_accuracy: 0.6116\n"
     ]
    }
   ],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "history=model.fit(xtrain_pad, y=y_train_enc_nn, batch_size=25, epochs=300, verbose=1, validation_data=(xtest_pad, y_test_enc_nn),callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "iemocap EmotionfromConv.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
